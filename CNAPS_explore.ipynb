{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a3edc9f-3c66-4e03-8c75-b147f15b2e50",
   "metadata": {},
   "source": [
    "# Explore the CNAPS US East Coast and Gulf of Mexico Forecast Archive Dataset\n",
    "This is a cloud-optimized version of the NetCDF files accessed. The original daily forecast files were converted into weekly NetCDF files with 168 points in the time dimension to facilitate time series access. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "547b851f-3dd9-4b9f-988b-54c743a43bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import fsspec\n",
    "import xarray as xr\n",
    "import hvplot.xarray\n",
    "import intake\n",
    "import cf_xarray\n",
    "import numpy as np\n",
    "import panel as pn\n",
    "from matplotlib import path\n",
    "import xoak"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57354df2-6786-4d1d-859e-1d5099cb85b9",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Open Dataset\n",
    "\n",
    "The details of data loading are stored in an `intake` catalog, which simplifies use.  Metadata and coordinate data are loaded, but not the actual data variables, which are loaded only as needed by subsequent analysis and visualization. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "067d1b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "intake_catalog_url = 'cnaps.yml'\n",
    "cat = intake.open_catalog(intake_catalog_url, storage_options={\"anon\": True} )\n",
    "list(cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d6c625-b5d7-4a17-ae2d-d7c2e8bc3fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'CNAPS_Forecast_Archive' "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59028b58-1a1a-4312-b8e1-ce11200eefef",
   "metadata": {},
   "source": [
    "This is a big dataset, so it takes up to 30s to open the dataset (which involves reading all the metadata and index coordinate variable data). Here we load the data into xarray using `.to_dask()` so that if we have a Dask cluster, we can speed up data processing by loading and processing chunks of data in parallel. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3b2ce8a-6499-4413-861c-65d48f7af108",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "ds = cat[dataset].to_dask()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cfe57c8-3892-4a02-9683-a7814e266b5a",
   "metadata": {},
   "source": [
    "Let's look at that metadata.  We can explore the different attributes and variables by clicking on the variables and icons below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "608b1398-bf35-40c8-bd9f-ff21238ae591",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15db8c13-9961-4be1-ab57-382d75f30999",
   "metadata": {},
   "source": [
    "We can also explore a specific variable of interest:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c72e31c-210a-4025-8293-478bb0f38b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "var = 'Hwave'\n",
    "da = ds[var]\n",
    "da"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd304dcd-079f-498a-bff6-d27cd9a118a7",
   "metadata": {},
   "source": [
    "Use the CF conventions to identify the coordinate variables for longitude, latitude and time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92f285e5-efc6-4952-b828-f3a882a8045e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "da = ds['Hwave']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90ff7b29-0c6f-4fa4-aa01-d10e2ee0e00a",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = da.cf['longitude']\n",
    "y = da.cf['latitude']\n",
    "t = da.cf['time']\n",
    "print(x.name, y.name, t.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e2d694-6371-4714-8dcd-11088fd32555",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "da.sel(ocean_time='2012-10-29 12:00', method='nearest')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e736ff3-b0e4-4452-af46-1afa462af185",
   "metadata": {},
   "source": [
    "## Example A: Load the entire spatial domain for a variable at a specific time step\n",
    "Loading the entire spatial domain \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f24af824-6395-4a40-add9-2ed6e4b507c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "sel = {t.name:'2012-10-29 12:00', 'method':'nearest'}\n",
    "da2d = da.sel(**sel).load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f46edc95-8904-4256-9729-7c09f03c51d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "da2d.hvplot.quadmesh(x=x.name, y=y.name, rasterize=True, geo=True, tiles='OSM', cmap='viridis')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91c71dae-3e8e-4293-b741-9c0992cc4c52",
   "metadata": {},
   "source": [
    "## Example B: Load a time series for a variable at a specific lon,lat location for a specified time range. \n",
    "We can see how many chunks of data must be loaded to extract a time series inspecting the dataset that will result:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecef85f6-01e4-4d84-9cce-0bf409b30637",
   "metadata": {},
   "source": [
    "### Parallelize with Dask \n",
    "We opened the dataset so that we can take advantage of parallel compute environments\n",
    "using `dask`. We're going to start a cluster now so that future steps can take advantage\n",
    "of this ability. \n",
    "\n",
    "This is an optional step, but speeds up data loading and processing significantly, especially \n",
    "when accessing data from the cloud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21a4346b-a786-41c8-9798-c088783b156e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# start cluster using helper routine from /shared/users/lib/zambon.py\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(os.path.join(os.environ['HOME'],'shared','users','lib'))\n",
    "import zambon as joe\n",
    "\n",
    "worker_max = 10\n",
    "client,cluster = joe.start_dask_cluster(profile=None, worker_max=worker_max, \n",
    "                                      region=None, use_existing_cluster=False,\n",
    "                                      adaptive_scaling=False, wait_for_cluster=True, \n",
    "                                      worker_profile='Small Worker', \n",
    "                                      propagate_env=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17ea39e1-14d9-4b85-bfe7-f3468b5220a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cluster.adapt(minimum=2, maximum=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e55a326-907c-4842-8090-b99723f0a31b",
   "metadata": {},
   "outputs": [],
   "source": [
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd35652-608d-4eca-94d9-78cfd3591b38",
   "metadata": {},
   "source": [
    "To identify a point, we will start with its lat/lon coordinates.  If lon and lat were 1D coordinates, we could use lon,lat values to select using xarray, but instead we need to extract using indices, which we need to find.   For this we use the `xoak` package:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e1bae3e-54dc-46fd-8d52-c8bd48719285",
   "metadata": {},
   "outputs": [],
   "source": [
    "lat,lon = 42.0, -69.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "654b797b-5720-49ac-8376-9f64a9113bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "da.xoak.set_index([y.name, x.name], 'scipy_kdtree')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c13c1276-1390-4ab3-ae64-bf77fae19327",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_point = xr.Dataset({\"lon\": (\"point\", [lon]), \"lat\": (\"point\", [lat])})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a7a4047-2104-423b-bdae-edf1a623c3d8",
   "metadata": {},
   "source": [
    "Load one month:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53bbf395-9ea9-4ccb-810e-42679738015f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "ds_selection = da.xoak.sel(lat_rho=ds_point.lat, lon_rho=ds_point.lon).sel(ocean_time='2023-10').load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "748ad474-43af-443f-9809-68f99f84d347",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_selection.hvplot(x=t.name, grid=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f45e3c27-2002-488a-b185-146df1307054",
   "metadata": {},
   "source": [
    "Load the entire time series:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4695724b-dbaf-4d2a-9343-fa0077ccbd58",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "ds_selection = da.xoak.sel(lat_rho=ds_point.lat, lon_rho=ds_point.lon).load()\n",
    "ds_selection.hvplot(x=t.name, grid=True)                       "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0472ff9-ec80-43c3-8fb2-2d096415b002",
   "metadata": {},
   "source": [
    "## Example C: Compute the time mean for a variable over the entire domain for a specific time period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e094faaf-fd3f-45eb-a65c-c4c1adaf3c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "da_mean = da.sel(ocean_time=slice('2023-10-28 00:00','2023-11-01 00:00')).mean(dim='ocean_time').compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a77448c7-8c01-44b1-b4f7-f8fb64a90714",
   "metadata": {},
   "outputs": [],
   "source": [
    "da_mean.hvplot.quadmesh(x=x.name, y=y.name, rasterize=True, geo=True,  tiles='OSM', frame_width=600, cmap='viridis')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c415fcc6-fa98-44f1-8e17-99034f717cb8",
   "metadata": {},
   "source": [
    "## Example D: Subset a time and space region and export to NetCDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa229688-91ef-4337-a051-71299b3c17a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bbox2ij(lon,lat,bbox=[-160., -155., 18., 23.]):\n",
    "    \"\"\"Return indices for i,j that will completely cover the specified bounding box.     \n",
    "    i0,i1,j0,j1 = bbox2ij(lon,lat,bbox)\n",
    "    lon,lat = 2D arrays that are the target of the subset\n",
    "    bbox = list containing the bounding box: [lon_min, lon_max, lat_min, lat_max]\n",
    "\n",
    "    Example\n",
    "    -------  \n",
    "    >>> i0,i1,j0,j1 = bbox2ij(lon_rho,lat_rho,[-71, -63., 39., 46])\n",
    "    >>> h_subset = nc.variables['h'][j0:j1,i0:i1]       \n",
    "    \"\"\"\n",
    "    bbox=np.array(bbox)\n",
    "    mypath=np.array([bbox[[0,1,1,0]],bbox[[2,2,3,3]]]).T\n",
    "    p = path.Path(mypath)\n",
    "    points = np.vstack((lon.ravel(),lat.ravel())).T   \n",
    "    n,m = np.shape(lon)\n",
    "    inside = p.contains_points(points).reshape((n,m))\n",
    "    ii,jj = np.meshgrid(range(m),range(n))\n",
    "    return min(ii[inside]),max(ii[inside]),min(jj[inside]),max(jj[inside])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85b4ba2a-1385-4002-b7e6-596b2b5025b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "bbox = [-76.63290610753754, -73.55671530588432, 37.57888442021855, 41.225532965406224]   # DRB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d0659c1-2757-43d6-9289-be55c4a39daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "i0,i1,j0,j1 = bbox2ij(x.values, y.values, bbox=bbox)\n",
    "print(i0,i1,j0,j1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee6e947-afbe-4b2b-88ff-59e3f84ef239",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_drb = ds[['temp', 'salt', 'Hwave']].isel(eta_rho=slice(j0,j1), xi_rho=slice(i0,i1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8cca25f-ef67-4f0a-9cc9-a4f7d4bd1add",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_drb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f9f2eff-25c6-46ec-b735-7ec57195db41",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_drb_timeslice = ds_drb.sel(ocean_time=slice('2023-10-28 00:00','2023-11-01 00:00'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f35132f-3881-449e-b4e0-8178beeaacfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_drb_timeslice = ds_drb_timeslice.chunk({'eta_rho':-1, 'xi_rho':-1})  # chunk to full spatial subset domain\n",
    "print(f'Uncompressed dataset size: {ds_drb_timeslice.nbytes/1e6} MB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66c55baf-1388-48ad-a1c4-fa13b063f1f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "var = 'Hwave'\n",
    "da_drb = ds_drb_timeslice[var]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "336eb5a6-9f2e-4449-87cb-6bae27ef689c",
   "metadata": {},
   "source": [
    "Close the Dask client, as we can't write NetCDF in parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82438a16-d3a3-4acc-aac6-f7700b28be27",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "client.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6067684-8781-43b0-aacd-308e8a32709d",
   "metadata": {},
   "outputs": [],
   "source": [
    "viz = da_drb.hvplot.quadmesh(x=x.name, y=y.name, geo=True, frame_width=400,\n",
    "                    cmap='turbo', rasterize=True, tiles='OSM', title=var)\n",
    "viz = pn.panel(viz, widgets={'ocean_time': pn.widgets.Select} )\n",
    "pn.Column(viz).servable('DRB Explorer')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1ed31c0-a1af-4ae1-a4ce-aa7e9977dec0",
   "metadata": {},
   "source": [
    "Specify encoding to enable compression in NetCDF file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a08d724-1f12-4c94-bd29-9addc5d45e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding={}\n",
    "for var in ds_drb_timeslice.variables:\n",
    "    encoding[var] = dict(zlib=True, complevel=4, \n",
    "                         fletcher32=False, shuffle=True,\n",
    "                         _FillValue=None)                        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21b6c948-84c2-4c26-822f-52dba14cbde0",
   "metadata": {},
   "source": [
    "Write the NetCDF file to local disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af926d53-4b79-472e-902f-622ee957f99a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "ds_drb_timeslice.to_netcdf('drb.nc', encoding=encoding, mode='w', engine='h5netcdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a999704-ffbc-4204-b127-58f16ea29865",
   "metadata": {},
   "source": [
    "Check size of resulting file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b4a059-c3e8-4218-a4a8-5f87dba56011",
   "metadata": {},
   "outputs": [],
   "source": [
    "fs = fsspec.filesystem('file', skip_instance_cache=True)\n",
    "fsize = fs.size('drb.nc')/1e6\n",
    "print(f'NetCDF file size: {fsize} MB')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9230a094-4dcd-4552-852e-be62cb29e15d",
   "metadata": {},
   "source": [
    "## Stop cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3aff08b-975d-47e1-ac51-6bb8e8b1adbc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "client.close(); cluster.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd416076-6840-43da-975c-35a63eef84b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e28ce718-dd59-40cd-bedb-504c7cc5e576",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nebari-git-nebari-git-pangeo2",
   "language": "python",
   "name": "conda-env-nebari-git-nebari-git-pangeo2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
